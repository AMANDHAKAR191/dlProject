{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\n# hyperparameters\nbatch_size = 16 # how many independent sequences will we process in parallel?\nblock_size = 32 # what is the maximum context length for predictions?\nmax_iters = 5000\neval_interval = 100\nlearning_rate = 1e-3\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\neval_iters = 200\nn_embd = 64\nn_head = 4\nn_layer = 4\ndropout = 0.0\n# ------------\n\ntorch.manual_seed(1337)\n\n!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\nwith open('input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\n\n# here are all the unique characters that occur in this text\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\n# create a mapping from characters to integers\nstoi = { ch:i for i,ch in enumerate(chars) }\nitos = { i:ch for i,ch in enumerate(chars) }\nencode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\ndecode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n\n# Train and test splits\ndata = torch.tensor(encode(text), dtype=torch.long)\nn = int(0.9*len(data)) # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]\n\n# data loading\ndef get_batch(split):\n    # generate a small batch of data of inputs x and targets y\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    x, y = x.to(device), y.to(device)\n    return x, y\n\n@torch.no_grad()\ndef estimate_loss():\n    out = {}\n    model.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            X, Y = get_batch(split)\n            logits, loss = model(X, Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    model.train()\n    return out\n\nclass Head(nn.Module):\n    \"\"\" one head of self-attention \"\"\"\n\n    def __init__(self, head_size):\n        super().__init__()\n        self.key = nn.Linear(n_embd, head_size, bias=False)\n        self.query = nn.Linear(n_embd, head_size, bias=False)\n        self.value = nn.Linear(n_embd, head_size, bias=False)\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        B,T,C = x.shape\n        k = self.key(x)   # (B,T,C)\n        q = self.query(x) # (B,T,C)\n        # compute attention scores (\"affinities\")\n        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n        wei = F.softmax(wei, dim=-1) # (B, T, T)\n        wei = self.dropout(wei)\n        # perform the weighted aggregation of the values\n        v = self.value(x) # (B,T,C)\n        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n        return out\n\nclass MultiHeadAttention(nn.Module):\n    \"\"\" multiple heads of self-attention in parallel \"\"\"\n\n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n        self.proj = nn.Linear(n_embd, n_embd)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim=-1)\n        out = self.dropout(self.proj(out))\n        return out\n\nclass FeedFoward(nn.Module):\n    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n\n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, 4 * n_embd),\n            nn.ReLU(),\n            nn.Linear(4 * n_embd, n_embd),\n            nn.Dropout(dropout),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\nclass Block(nn.Module):\n    \"\"\" Transformer block: communication followed by computation \"\"\"\n\n    def __init__(self, n_embd, n_head):\n        # n_embd: embedding dimension, n_head: the number of heads we'd like\n        super().__init__()\n        head_size = n_embd // n_head\n        self.sa = MultiHeadAttention(n_head, head_size)\n        self.ffwd = FeedFoward(n_embd)\n        self.ln1 = nn.LayerNorm(n_embd)\n        self.ln2 = nn.LayerNorm(n_embd)\n\n    def forward(self, x):\n        x = x + self.sa(self.ln1(x))\n        x = x + self.ffwd(self.ln2(x))\n        return x\n\n# super simple bigram model\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n\n        # idx and targets are both (B,T) tensor of integers\n        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n        x = tok_emb + pos_emb # (B,T,C)\n        x = self.blocks(x) # (B,T,C)\n        x = self.ln_f(x) # (B,T,C)\n        logits = self.lm_head(x) # (B,T,vocab_size)\n\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        # idx is (B, T) array of indices in the current context\n        for _ in range(max_new_tokens):\n            # crop idx to the last block_size tokens\n            idx_cond = idx[:, -block_size:]\n            # get the predictions\n            logits, loss = self(idx_cond)\n            # focus only on the last time step\n            logits = logits[:, -1, :] # becomes (B, C)\n            # apply softmax to get probabilities\n            probs = F.softmax(logits, dim=-1) # (B, C)\n            # sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n            # append sampled index to the running sequence\n            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n        return idx\n\nmodel = BigramLanguageModel()\nm = model.to(device)\n# print the number of parameters in the model\nprint(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n\n# create a PyTorch optimizer\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n\nfor iter in range(max_iters):\n\n    # every once in a while evaluate the loss on train and val sets\n    if iter % eval_interval == 0 or iter == max_iters - 1:\n        losses = estimate_loss()\n        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n\n    # sample a batch of data\n    xb, yb = get_batch('train')\n\n    # evaluate the loss\n    logits, loss = model(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-05-01T05:10:46.825512Z","iopub.execute_input":"2023-05-01T05:10:46.826634Z","iopub.status.idle":"2023-05-01T05:15:23.051990Z","shell.execute_reply.started":"2023-05-01T05:10:46.826586Z","shell.execute_reply":"2023-05-01T05:15:23.050368Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"--2023-05-01 05:10:47--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.108.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 1115394 (1.1M) [text/plain]\nSaving to: ‘input.txt.1’\n\ninput.txt.1         100%[===================>]   1.06M  --.-KB/s    in 0.06s   \n\n2023-05-01 05:10:47 (19.1 MB/s) - ‘input.txt.1’ saved [1115394/1115394]\n\n0.209729 M parameters\nstep 0: train loss 4.4116, val loss 4.4022\nstep 100: train loss 2.6568, val loss 2.6670\nstep 200: train loss 2.5091, val loss 2.5059\nstep 300: train loss 2.4194, val loss 2.4334\nstep 400: train loss 2.3503, val loss 2.3568\nstep 500: train loss 2.2961, val loss 2.3128\nstep 600: train loss 2.2407, val loss 2.2499\nstep 700: train loss 2.2046, val loss 2.2183\nstep 800: train loss 2.1640, val loss 2.1873\nstep 900: train loss 2.1237, val loss 2.1502\nstep 1000: train loss 2.1022, val loss 2.1294\nstep 1100: train loss 2.0697, val loss 2.1177\nstep 1200: train loss 2.0378, val loss 2.0783\nstep 1300: train loss 2.0247, val loss 2.0631\nstep 1400: train loss 1.9916, val loss 2.0362\nstep 1500: train loss 1.9693, val loss 2.0302\nstep 1600: train loss 1.9639, val loss 2.0483\nstep 1700: train loss 1.9428, val loss 2.0148\nstep 1800: train loss 1.9066, val loss 1.9939\nstep 1900: train loss 1.9093, val loss 1.9900\nstep 2000: train loss 1.8859, val loss 1.9962\nstep 2100: train loss 1.8696, val loss 1.9732\nstep 2200: train loss 1.8606, val loss 1.9629\nstep 2300: train loss 1.8574, val loss 1.9542\nstep 2400: train loss 1.8427, val loss 1.9455\nstep 2500: train loss 1.8182, val loss 1.9449\nstep 2600: train loss 1.8267, val loss 1.9396\nstep 2700: train loss 1.8105, val loss 1.9322\nstep 2800: train loss 1.8028, val loss 1.9205\nstep 2900: train loss 1.8048, val loss 1.9303\nstep 3000: train loss 1.7940, val loss 1.9173\nstep 3100: train loss 1.7714, val loss 1.9192\nstep 3200: train loss 1.7551, val loss 1.9140\nstep 3300: train loss 1.7575, val loss 1.9094\nstep 3400: train loss 1.7526, val loss 1.8957\nstep 3500: train loss 1.7374, val loss 1.8937\nstep 3600: train loss 1.7257, val loss 1.8896\nstep 3700: train loss 1.7284, val loss 1.8874\nstep 3800: train loss 1.7186, val loss 1.8865\nstep 3900: train loss 1.7199, val loss 1.8720\nstep 4000: train loss 1.7072, val loss 1.8552\nstep 4100: train loss 1.7093, val loss 1.8718\nstep 4200: train loss 1.7040, val loss 1.8581\nstep 4300: train loss 1.6991, val loss 1.8484\nstep 4400: train loss 1.7045, val loss 1.8602\nstep 4500: train loss 1.6865, val loss 1.8476\nstep 4600: train loss 1.6879, val loss 1.8340\nstep 4700: train loss 1.6829, val loss 1.8431\nstep 4800: train loss 1.6641, val loss 1.8427\nstep 4900: train loss 1.6711, val loss 1.8387\nstep 4999: train loss 1.6620, val loss 1.8234\n\n\nYORK:\nBRATHNy will, O loves that seek obe to take O-dam the chause:\nWart I husque, to barte lassay, away, my fears' comzons heavens\nMakfult her! Varwhed lib, egried,\nEvencie; at Heriliov the done me now one was oneldring tear.\nHoust confiry wish have thee yet:\nThat I crown this deserity sworly that\nmont-now where I baste at too rive with is his sope\nAs forgune kindnuntury for armion to shall do flear the pade.\nFeed Some, Her, be!\nAt you as ards. Warwick the huis courtear tey, repts I cromfort young to the marrition one your mest a ceess to permant!\nNot throany whom such swonly were one.\n\nTYMy Marsper:\nStay might. \nCLAMENRY:\nMy where'seles Peering ove maded as caturess.\nMy vittress so upon sixe feer, as not contlemms\nTunnies, is duke in, so may I want bear\nthat fravius wrants slevors. O now, to scrult I?\n\nGRICEY:\nMy honours?\nTheir him the name, drue?\nFyreet I doou, is onterrning tears\nBe wipprings is be than the must perelool keeds to him\nFor for me watch time to the set us\nyou he bout on than you for allight\nOf Rizing. where conve, do prive, and none:\nWay honsurage, be, mest things seements\nSinging, for I have Warwaw whide,\nAn bear was all beenIn\nBefore and to the severeign Pirmplity lament tell the Regreast men is what seeping\nTo settas stiond's leve,\nShame us than prive, forth, not just deer'd,\nStraught?----ARTANNA:\nWhy is is stadve.\n\nProy Wondume to your hurse beguts\nGood vonsure to you, must a be; I now.\n\nBRAMBUS:\nWhat may'ster, speak.\n\nLUCHIUS:\nWhile these art: ullis thee, smeet on you;\nHow plow the gends; fortune of a--a scace:\nOut.\n\nPOMALLINA:\nWhere\nOur wrange breets, you will him strunkning'd that to that may\nAn I shad arwild-farking.\n\nBRABETH:\n\nFirht:\nOur as is Prestions:\nI have bloy not, there wife it: sup that heave our cittutom.\nThe linowly talker at the swormn'd,\nTalk'd our twould you her, to life?\n\nCOMINIUS:\nYou hove cause; 'tis good not yone your mond. I, am-nseat, Joancet:\nThe will on the Riparding: I wills men's fortunds, sue:\nTo firther, who hatw \n","output_type":"stream"}]},{"cell_type":"code","source":"# generate from the model\n# Define input string\ninput_string = \"Hello, there\"\n# Encode input string into character indices\ninput_indices = encode(input_string)\n# Convert list of indices into a tensor with shape (1, len(input_indices))\ncontext = torch.tensor([input_indices], dtype=torch.long, device=device)\n\n# Generate text using the modified context\nprint(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n","metadata":{"execution":{"iopub.status.busy":"2023-05-01T05:29:50.788917Z","iopub.execute_input":"2023-05-01T05:29:50.789937Z","iopub.status.idle":"2023-05-01T05:29:54.101183Z","shell.execute_reply.started":"2023-05-01T05:29:50.789895Z","shell.execute_reply":"2023-05-01T05:29:54.100001Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Hello, there favale ea adth.\n\nHENRY MAGIUS:\nWhat csee,\nAnd I have must stricke sentrom before: I will, do, my dratutent\nAnd the suit claursel for plood:\nMy rever hate would to against\nQume whom you I duest putraining sab,\nOnve would by this glift's nobe some on thy hund!\nWhat seesser Foriar then all,\nI'llow be caure of diders of\nby provose for you honter agarm and srante the fwilt be\nTo seers, to lip-truef; that re'er unperter\nSo my my fair thus art fattping.\n\nBRUTUS:\nThe my hengely as rispower'd to thy\nso \n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}