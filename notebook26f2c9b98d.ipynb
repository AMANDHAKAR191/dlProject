{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nimport urllib.request\n\n# hyperparameters\nbatch_size = 16 # how many independent sequences will we process in parallel?\nblock_size = 32 # what is the maximum context length for predictions?\nmax_iters = 40000\neval_interval = 100\nlearning_rate = 1e-3\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\neval_iters = 200\nn_embd = 64\nn_head = 4\nn_layer = 4\ndropout = 0.0\n# ------------\n\ntorch.manual_seed(1337)\n\n\n\n# Download the tiny shakespeare dataset\nurl = \"https://raw.githubusercontent.com/AMANDHAKAR191/dlProject/main/dataset_combined.txt\"\nfilename = \"dataset_combined.txt\"\n\nurllib.request.urlretrieve(url, filename)\n\n# Read the dataset to inspect it\nwith open(filename, 'r', encoding='utf-8') as f:\n    text = f.read()\n\n# Print the length of the dataset in characters and the first 1000 characters\nprint(\"Length of dataset in characters: \", len(text))\nprint(text[:1000])\n\n\n# here are all the unique characters that occur in this text\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\n# create a mapping from characters to integers\nstoi = { ch:i for i,ch in enumerate(chars) }\nitos = { i:ch for i,ch in enumerate(chars) }\nencode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\ndecode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n\n# Train and test splits\ndata = torch.tensor(encode(text), dtype=torch.long)\nn = int(0.9*len(data)) # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]\n\n# data loading\ndef get_batch(split):\n    # generate a small batch of data of inputs x and targets y\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    x, y = x.to(device), y.to(device)\n    return x, y\n\n@torch.no_grad()\ndef estimate_loss():\n    out = {}\n    model.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            X, Y = get_batch(split)\n            logits, loss = model(X, Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    model.train()\n    return out\n\nclass Head(nn.Module):\n    \"\"\" one head of self-attention \"\"\"\n\n    def __init__(self, head_size):\n        super().__init__()\n        self.key = nn.Linear(n_embd, head_size, bias=False)\n        self.query = nn.Linear(n_embd, head_size, bias=False)\n        self.value = nn.Linear(n_embd, head_size, bias=False)\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        B,T,C = x.shape\n        k = self.key(x)   # (B,T,C)\n        q = self.query(x) # (B,T,C)\n        # compute attention scores (\"affinities\")\n        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n        wei = F.softmax(wei, dim=-1) # (B, T, T)\n        wei = self.dropout(wei)\n        # perform the weighted aggregation of the values\n        v = self.value(x) # (B,T,C)\n        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n        return out\n\nclass MultiHeadAttention(nn.Module):\n    \"\"\" multiple heads of self-attention in parallel \"\"\"\n\n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n        self.proj = nn.Linear(n_embd, n_embd)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim=-1)\n        out = self.dropout(self.proj(out))\n        return out\n\nclass FeedFoward(nn.Module):\n    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n\n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, 4 * n_embd),\n            nn.ReLU(),\n            nn.Linear(4 * n_embd, n_embd),\n            nn.Dropout(dropout),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\nclass Block(nn.Module):\n    \"\"\" Transformer block: communication followed by computation \"\"\"\n\n    def __init__(self, n_embd, n_head):\n        # n_embd: embedding dimension, n_head: the number of heads we'd like\n        super().__init__()\n        head_size = n_embd // n_head\n        self.sa = MultiHeadAttention(n_head, head_size)\n        self.ffwd = FeedFoward(n_embd)\n        self.ln1 = nn.LayerNorm(n_embd)\n        self.ln2 = nn.LayerNorm(n_embd)\n\n    def forward(self, x):\n        x = x + self.sa(self.ln1(x))\n        x = x + self.ffwd(self.ln2(x))\n        return x\n\n# super simple bigram model\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n\n        # idx and targets are both (B,T) tensor of integers\n        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n        x = tok_emb + pos_emb # (B,T,C)\n        x = self.blocks(x) # (B,T,C)\n        x = self.ln_f(x) # (B,T,C)\n        logits = self.lm_head(x) # (B,T,vocab_size)\n\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        # idx is (B, T) array of indices in the current context\n        for _ in range(max_new_tokens):\n            # crop idx to the last block_size tokens\n            idx_cond = idx[:, -block_size:]\n            # get the predictions\n            logits, loss = self(idx_cond)\n            # focus only on the last time step\n            logits = logits[:, -1, :] # becomes (B, C)\n            # apply softmax to get probabilities\n            probs = F.softmax(logits, dim=-1) # (B, C)\n            # sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n            # append sampled index to the running sequence\n            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n        return idx\n\nmodel = BigramLanguageModel()\nm = model.to(device)\n# print the number of parameters in the model\nprint(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n\n# create a PyTorch optimizer\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n\nfor iter in range(max_iters):\n\n    # every once in a while evaluate the loss on train and val sets\n    if iter % eval_interval == 0 or iter == max_iters - 1:\n        losses = estimate_loss()\n        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n\n    # sample a batch of data\n    xb, yb = get_batch('train')\n\n    # evaluate the loss\n    logits, loss = model(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-05-01T09:02:21.787995Z","iopub.execute_input":"2023-05-01T09:02:21.788288Z","iopub.status.idle":"2023-05-01T09:37:13.826621Z","shell.execute_reply.started":"2023-05-01T09:02:21.788259Z","shell.execute_reply":"2023-05-01T09:37:13.825379Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Length of dataset in characters:  2861882\nBLACK PANTHER\n\n                         \n\n                          \n                           Written by \n\n                 Ryan Coogler & Joe Robert Cole\n\n\n\n\n\n\n          EXT. DEEP SPACE\n\n          A dark screen is lit up by twinkling stars .\n\n                          SON\n           Baba?\n\n                          FATHER\n           Yes, my son?\n\n                          SON\n           Tell me a story .\n\n                          FATHER\n           Which one?\n\n                          SON\n           The story of home .\n          A meteorite drifts into frame , heading towards tiny Earth off\n          in the distance.\n\n                          FATHER\n           Millions of years ago , a meteorite\n           made of vibranium, the strongest\n           substance in the universe struck\n           the continent of Africa affecting\n           the plant life around it.\n\n          The meteorite hits Africa and we see plant life and animals\n          affected by vibranium.\n\n               \n0.213341 M parameters\nstep 0: train loss 4.4650, val loss 4.6406\nstep 100: train loss 2.4232, val loss 2.8122\nstep 200: train loss 2.2147, val loss 2.6425\nstep 300: train loss 2.1456, val loss 2.5586\nstep 400: train loss 2.0894, val loss 2.5213\nstep 500: train loss 2.0309, val loss 2.4514\nstep 600: train loss 2.0061, val loss 2.4017\nstep 700: train loss 1.9774, val loss 2.3545\nstep 800: train loss 1.9490, val loss 2.3449\nstep 900: train loss 1.9117, val loss 2.2935\nstep 1000: train loss 1.8697, val loss 2.2686\nstep 1100: train loss 1.8867, val loss 2.2684\nstep 1200: train loss 1.8673, val loss 2.2435\nstep 1300: train loss 1.8199, val loss 2.2174\nstep 1400: train loss 1.8149, val loss 2.1852\nstep 1500: train loss 1.8158, val loss 2.1890\nstep 1600: train loss 1.7845, val loss 2.1667\nstep 1700: train loss 1.7993, val loss 2.1487\nstep 1800: train loss 1.7557, val loss 2.1425\nstep 1900: train loss 1.7624, val loss 2.1235\nstep 2000: train loss 1.7465, val loss 2.1120\nstep 2100: train loss 1.7553, val loss 2.1140\nstep 2200: train loss 1.7169, val loss 2.0815\nstep 2300: train loss 1.7242, val loss 2.0865\nstep 2400: train loss 1.7024, val loss 2.0690\nstep 2500: train loss 1.6948, val loss 2.0723\nstep 2600: train loss 1.7096, val loss 2.0682\nstep 2700: train loss 1.6666, val loss 2.0562\nstep 2800: train loss 1.6719, val loss 2.0564\nstep 2900: train loss 1.6564, val loss 2.0255\nstep 3000: train loss 1.6587, val loss 2.0291\nstep 3100: train loss 1.6441, val loss 2.0286\nstep 3200: train loss 1.6588, val loss 2.0088\nstep 3300: train loss 1.6514, val loss 2.0202\nstep 3400: train loss 1.6535, val loss 1.9921\nstep 3500: train loss 1.6267, val loss 2.0064\nstep 3600: train loss 1.6273, val loss 1.9877\nstep 3700: train loss 1.6039, val loss 1.9850\nstep 3800: train loss 1.6099, val loss 1.9716\nstep 3900: train loss 1.6293, val loss 1.9697\nstep 4000: train loss 1.6270, val loss 1.9613\nstep 4100: train loss 1.6036, val loss 1.9591\nstep 4200: train loss 1.5954, val loss 1.9515\nstep 4300: train loss 1.5928, val loss 1.9522\nstep 4400: train loss 1.6061, val loss 1.9592\nstep 4500: train loss 1.5870, val loss 1.9421\nstep 4600: train loss 1.5799, val loss 1.9429\nstep 4700: train loss 1.5695, val loss 1.9225\nstep 4800: train loss 1.5827, val loss 1.9513\nstep 4900: train loss 1.5657, val loss 1.9318\nstep 5000: train loss 1.5620, val loss 1.9300\nstep 5100: train loss 1.5457, val loss 1.9126\nstep 5200: train loss 1.5447, val loss 1.9138\nstep 5300: train loss 1.5543, val loss 1.8991\nstep 5400: train loss 1.5406, val loss 1.8947\nstep 5500: train loss 1.5611, val loss 1.9086\nstep 5600: train loss 1.5388, val loss 1.8931\nstep 5700: train loss 1.5513, val loss 1.9007\nstep 5800: train loss 1.5209, val loss 1.8996\nstep 5900: train loss 1.5577, val loss 1.8952\nstep 6000: train loss 1.5378, val loss 1.8922\nstep 6100: train loss 1.5347, val loss 1.8891\nstep 6200: train loss 1.5143, val loss 1.8812\nstep 6300: train loss 1.5228, val loss 1.9080\nstep 6400: train loss 1.5431, val loss 1.8824\nstep 6500: train loss 1.5222, val loss 1.8760\nstep 6600: train loss 1.5258, val loss 1.8745\nstep 6700: train loss 1.4906, val loss 1.8680\nstep 6800: train loss 1.5141, val loss 1.8654\nstep 6900: train loss 1.5283, val loss 1.8660\nstep 7000: train loss 1.5225, val loss 1.8552\nstep 7100: train loss 1.5073, val loss 1.8706\nstep 7200: train loss 1.5142, val loss 1.8635\nstep 7300: train loss 1.5072, val loss 1.8565\nstep 7400: train loss 1.4941, val loss 1.8493\nstep 7500: train loss 1.4834, val loss 1.8417\nstep 7600: train loss 1.4789, val loss 1.8381\nstep 7700: train loss 1.4908, val loss 1.8502\nstep 7800: train loss 1.5135, val loss 1.8551\nstep 7900: train loss 1.4896, val loss 1.8547\nstep 8000: train loss 1.4876, val loss 1.8343\nstep 8100: train loss 1.4752, val loss 1.8438\nstep 8200: train loss 1.4785, val loss 1.8370\nstep 8300: train loss 1.4724, val loss 1.8361\nstep 8400: train loss 1.4788, val loss 1.8375\nstep 8500: train loss 1.4874, val loss 1.8305\nstep 8600: train loss 1.5015, val loss 1.8253\nstep 8700: train loss 1.4736, val loss 1.8561\nstep 8800: train loss 1.4823, val loss 1.8247\nstep 8900: train loss 1.4711, val loss 1.8183\nstep 9000: train loss 1.4956, val loss 1.8231\nstep 9100: train loss 1.4525, val loss 1.8265\nstep 9200: train loss 1.4513, val loss 1.8281\nstep 9300: train loss 1.4621, val loss 1.8207\nstep 9400: train loss 1.4668, val loss 1.8190\nstep 9500: train loss 1.4566, val loss 1.8165\nstep 9600: train loss 1.4716, val loss 1.8168\nstep 9700: train loss 1.4653, val loss 1.8219\nstep 9800: train loss 1.4523, val loss 1.8276\nstep 9900: train loss 1.4649, val loss 1.8095\nstep 10000: train loss 1.4770, val loss 1.8165\nstep 10100: train loss 1.4640, val loss 1.8129\nstep 10200: train loss 1.4597, val loss 1.8117\nstep 10300: train loss 1.4507, val loss 1.8014\nstep 10400: train loss 1.4539, val loss 1.8139\nstep 10500: train loss 1.4548, val loss 1.8101\nstep 10600: train loss 1.4344, val loss 1.8080\nstep 10700: train loss 1.4514, val loss 1.8139\nstep 10800: train loss 1.4542, val loss 1.8127\nstep 10900: train loss 1.4580, val loss 1.7941\nstep 11000: train loss 1.4424, val loss 1.8029\nstep 11100: train loss 1.4434, val loss 1.8021\nstep 11200: train loss 1.4300, val loss 1.8046\nstep 11300: train loss 1.4816, val loss 1.8107\nstep 11400: train loss 1.4375, val loss 1.7952\nstep 11500: train loss 1.4669, val loss 1.7952\nstep 11600: train loss 1.4263, val loss 1.7992\nstep 11700: train loss 1.4468, val loss 1.8012\nstep 11800: train loss 1.4417, val loss 1.7948\nstep 11900: train loss 1.4265, val loss 1.7856\nstep 12000: train loss 1.4261, val loss 1.7826\nstep 12100: train loss 1.4428, val loss 1.7934\nstep 12200: train loss 1.4379, val loss 1.7764\nstep 12300: train loss 1.4282, val loss 1.7873\nstep 12400: train loss 1.4457, val loss 1.7922\nstep 12500: train loss 1.4442, val loss 1.7757\nstep 12600: train loss 1.4216, val loss 1.7951\nstep 12700: train loss 1.4273, val loss 1.7969\nstep 12800: train loss 1.4154, val loss 1.7820\nstep 12900: train loss 1.4301, val loss 1.7802\nstep 13000: train loss 1.4254, val loss 1.7806\nstep 13100: train loss 1.4223, val loss 1.7829\nstep 13200: train loss 1.4267, val loss 1.7763\nstep 13300: train loss 1.4220, val loss 1.7769\nstep 13400: train loss 1.4077, val loss 1.7865\nstep 13500: train loss 1.4217, val loss 1.7755\nstep 13600: train loss 1.4378, val loss 1.7696\nstep 13700: train loss 1.4221, val loss 1.7880\nstep 13800: train loss 1.4127, val loss 1.7589\nstep 13900: train loss 1.4184, val loss 1.7701\nstep 14000: train loss 1.4347, val loss 1.7786\nstep 14100: train loss 1.4208, val loss 1.7690\nstep 14200: train loss 1.4011, val loss 1.7687\nstep 14300: train loss 1.4168, val loss 1.7960\nstep 14400: train loss 1.3974, val loss 1.7607\nstep 14500: train loss 1.4413, val loss 1.7769\nstep 14600: train loss 1.4113, val loss 1.7568\nstep 14700: train loss 1.4207, val loss 1.7679\nstep 14800: train loss 1.4309, val loss 1.7700\nstep 14900: train loss 1.4126, val loss 1.7652\nstep 15000: train loss 1.4061, val loss 1.7608\nstep 15100: train loss 1.4135, val loss 1.7708\nstep 15200: train loss 1.4152, val loss 1.7688\nstep 15300: train loss 1.4253, val loss 1.7792\nstep 15400: train loss 1.4137, val loss 1.7633\nstep 15500: train loss 1.4245, val loss 1.7677\nstep 15600: train loss 1.4243, val loss 1.7615\nstep 15700: train loss 1.4019, val loss 1.7564\nstep 15800: train loss 1.4216, val loss 1.7551\nstep 15900: train loss 1.4143, val loss 1.7540\nstep 16000: train loss 1.4023, val loss 1.7548\nstep 16100: train loss 1.3991, val loss 1.7648\nstep 16200: train loss 1.4000, val loss 1.7603\nstep 16300: train loss 1.4098, val loss 1.7613\nstep 16400: train loss 1.4128, val loss 1.7440\nstep 16500: train loss 1.4160, val loss 1.7550\nstep 16600: train loss 1.4011, val loss 1.7660\nstep 16700: train loss 1.4087, val loss 1.7485\nstep 16800: train loss 1.4100, val loss 1.7483\nstep 16900: train loss 1.3996, val loss 1.7467\nstep 17000: train loss 1.3977, val loss 1.7417\nstep 17100: train loss 1.4074, val loss 1.7577\nstep 17200: train loss 1.3933, val loss 1.7453\nstep 17300: train loss 1.3935, val loss 1.7313\nstep 17400: train loss 1.3883, val loss 1.7417\nstep 17500: train loss 1.3971, val loss 1.7354\nstep 17600: train loss 1.3961, val loss 1.7408\nstep 17700: train loss 1.3917, val loss 1.7392\nstep 17800: train loss 1.3879, val loss 1.7525\nstep 17900: train loss 1.4047, val loss 1.7344\nstep 18000: train loss 1.3995, val loss 1.7413\nstep 18100: train loss 1.4023, val loss 1.7538\nstep 18200: train loss 1.3959, val loss 1.7430\nstep 18300: train loss 1.3963, val loss 1.7440\nstep 18400: train loss 1.3928, val loss 1.7306\nstep 18500: train loss 1.3860, val loss 1.7455\nstep 18600: train loss 1.4037, val loss 1.7433\nstep 18700: train loss 1.3967, val loss 1.7354\nstep 18800: train loss 1.4020, val loss 1.7360\nstep 18900: train loss 1.4072, val loss 1.7342\nstep 19000: train loss 1.4059, val loss 1.7509\nstep 19100: train loss 1.3908, val loss 1.7532\nstep 19200: train loss 1.3953, val loss 1.7388\nstep 19300: train loss 1.3917, val loss 1.7392\nstep 19400: train loss 1.3808, val loss 1.7418\nstep 19500: train loss 1.3970, val loss 1.7294\nstep 19600: train loss 1.3825, val loss 1.7392\nstep 19700: train loss 1.3884, val loss 1.7324\nstep 19800: train loss 1.3932, val loss 1.7266\nstep 19900: train loss 1.3854, val loss 1.7427\nstep 20000: train loss 1.3887, val loss 1.7338\nstep 20100: train loss 1.3923, val loss 1.7356\nstep 20200: train loss 1.3820, val loss 1.7379\nstep 20300: train loss 1.3801, val loss 1.7245\nstep 20400: train loss 1.3692, val loss 1.7243\nstep 20500: train loss 1.3849, val loss 1.7471\nstep 20600: train loss 1.3729, val loss 1.7367\nstep 20700: train loss 1.3797, val loss 1.7353\nstep 20800: train loss 1.3880, val loss 1.7323\nstep 20900: train loss 1.3824, val loss 1.7260\nstep 21000: train loss 1.3836, val loss 1.7382\nstep 21100: train loss 1.3725, val loss 1.7191\nstep 21200: train loss 1.3826, val loss 1.7397\nstep 21300: train loss 1.3683, val loss 1.7291\nstep 21400: train loss 1.3706, val loss 1.7086\nstep 21500: train loss 1.3683, val loss 1.7288\nstep 21600: train loss 1.3841, val loss 1.7195\nstep 21700: train loss 1.3833, val loss 1.7291\nstep 21800: train loss 1.3861, val loss 1.7336\nstep 21900: train loss 1.3742, val loss 1.7321\nstep 22000: train loss 1.3701, val loss 1.7291\nstep 22100: train loss 1.3829, val loss 1.7399\nstep 22200: train loss 1.3728, val loss 1.7259\nstep 22300: train loss 1.3771, val loss 1.7136\nstep 22400: train loss 1.3777, val loss 1.7330\nstep 22500: train loss 1.3832, val loss 1.7147\nstep 22600: train loss 1.3688, val loss 1.7178\nstep 22700: train loss 1.3818, val loss 1.7280\nstep 22800: train loss 1.3735, val loss 1.7263\nstep 22900: train loss 1.3563, val loss 1.7340\nstep 23000: train loss 1.3648, val loss 1.7216\nstep 23100: train loss 1.3510, val loss 1.7175\nstep 23200: train loss 1.3712, val loss 1.7253\nstep 23300: train loss 1.3727, val loss 1.7211\nstep 23400: train loss 1.3788, val loss 1.7251\nstep 23500: train loss 1.3704, val loss 1.7133\nstep 23600: train loss 1.3664, val loss 1.7146\nstep 23700: train loss 1.3796, val loss 1.7188\nstep 23800: train loss 1.3533, val loss 1.7115\nstep 23900: train loss 1.3774, val loss 1.7174\nstep 24000: train loss 1.3725, val loss 1.7333\nstep 24100: train loss 1.3642, val loss 1.6999\nstep 24200: train loss 1.3539, val loss 1.7158\nstep 24300: train loss 1.3545, val loss 1.7178\nstep 24400: train loss 1.3676, val loss 1.7094\nstep 24500: train loss 1.3661, val loss 1.7053\nstep 24600: train loss 1.3613, val loss 1.7195\nstep 24700: train loss 1.3530, val loss 1.7142\nstep 24800: train loss 1.3683, val loss 1.7157\nstep 24900: train loss 1.3528, val loss 1.7287\nstep 25000: train loss 1.3665, val loss 1.7154\nstep 25100: train loss 1.3666, val loss 1.7136\nstep 25200: train loss 1.3828, val loss 1.7215\nstep 25300: train loss 1.3815, val loss 1.7115\nstep 25400: train loss 1.3702, val loss 1.7134\nstep 25500: train loss 1.3514, val loss 1.7151\nstep 25600: train loss 1.3495, val loss 1.7118\nstep 25700: train loss 1.3611, val loss 1.7110\nstep 25800: train loss 1.3594, val loss 1.7133\nstep 25900: train loss 1.3586, val loss 1.7282\nstep 26000: train loss 1.3743, val loss 1.7190\nstep 26100: train loss 1.3543, val loss 1.7165\nstep 26200: train loss 1.3583, val loss 1.7097\nstep 26300: train loss 1.3623, val loss 1.7100\nstep 26400: train loss 1.3697, val loss 1.7042\nstep 26500: train loss 1.3622, val loss 1.7145\nstep 26600: train loss 1.3605, val loss 1.6949\nstep 26700: train loss 1.3773, val loss 1.7140\nstep 26800: train loss 1.3371, val loss 1.7101\nstep 26900: train loss 1.3410, val loss 1.7019\nstep 27000: train loss 1.3440, val loss 1.6903\nstep 27100: train loss 1.3611, val loss 1.7075\nstep 27200: train loss 1.3765, val loss 1.7115\nstep 27300: train loss 1.3506, val loss 1.7010\nstep 27400: train loss 1.3765, val loss 1.7079\nstep 27500: train loss 1.3665, val loss 1.6932\nstep 27600: train loss 1.3587, val loss 1.7035\nstep 27700: train loss 1.3582, val loss 1.6934\nstep 27800: train loss 1.3685, val loss 1.7217\nstep 27900: train loss 1.3716, val loss 1.7058\nstep 28000: train loss 1.3606, val loss 1.7127\nstep 28100: train loss 1.3318, val loss 1.7021\nstep 28200: train loss 1.3525, val loss 1.7024\nstep 28300: train loss 1.3481, val loss 1.7056\nstep 28400: train loss 1.3590, val loss 1.7016\nstep 28500: train loss 1.3444, val loss 1.7057\nstep 28600: train loss 1.3446, val loss 1.6900\nstep 28700: train loss 1.3442, val loss 1.7036\nstep 28800: train loss 1.3637, val loss 1.7042\nstep 28900: train loss 1.3645, val loss 1.7067\nstep 29000: train loss 1.3491, val loss 1.6969\nstep 29100: train loss 1.3553, val loss 1.6952\nstep 29200: train loss 1.3383, val loss 1.6965\nstep 29300: train loss 1.3737, val loss 1.7103\nstep 29400: train loss 1.3592, val loss 1.6984\nstep 29500: train loss 1.3492, val loss 1.7058\nstep 29600: train loss 1.3423, val loss 1.6989\nstep 29700: train loss 1.3381, val loss 1.6994\nstep 29800: train loss 1.3595, val loss 1.7015\nstep 29900: train loss 1.3580, val loss 1.6926\nstep 30000: train loss 1.3556, val loss 1.6982\nstep 30100: train loss 1.3704, val loss 1.6980\nstep 30200: train loss 1.3552, val loss 1.7012\nstep 30300: train loss 1.3363, val loss 1.7074\nstep 30400: train loss 1.3469, val loss 1.6942\nstep 30500: train loss 1.3563, val loss 1.6930\nstep 30600: train loss 1.3491, val loss 1.6906\nstep 30700: train loss 1.3584, val loss 1.6937\nstep 30800: train loss 1.3395, val loss 1.7045\nstep 30900: train loss 1.3510, val loss 1.6990\nstep 31000: train loss 1.3321, val loss 1.6912\nstep 31100: train loss 1.3285, val loss 1.6874\nstep 31200: train loss 1.3406, val loss 1.6973\nstep 31300: train loss 1.3533, val loss 1.7016\nstep 31400: train loss 1.3441, val loss 1.6851\nstep 31500: train loss 1.3368, val loss 1.7004\nstep 31600: train loss 1.3376, val loss 1.6956\nstep 31700: train loss 1.3490, val loss 1.6807\nstep 31800: train loss 1.3346, val loss 1.6913\nstep 31900: train loss 1.3492, val loss 1.7050\nstep 32000: train loss 1.3414, val loss 1.6953\nstep 32100: train loss 1.3366, val loss 1.6907\nstep 32200: train loss 1.3337, val loss 1.6885\nstep 32300: train loss 1.3292, val loss 1.6862\nstep 32400: train loss 1.3567, val loss 1.6958\nstep 32500: train loss 1.3657, val loss 1.7079\nstep 32600: train loss 1.3411, val loss 1.7088\nstep 32700: train loss 1.3439, val loss 1.7008\nstep 32800: train loss 1.3345, val loss 1.6878\nstep 32900: train loss 1.3374, val loss 1.7026\nstep 33000: train loss 1.3337, val loss 1.6904\nstep 33100: train loss 1.3308, val loss 1.6974\nstep 33200: train loss 1.3457, val loss 1.6903\nstep 33300: train loss 1.3273, val loss 1.6912\nstep 33400: train loss 1.3393, val loss 1.6843\nstep 33500: train loss 1.3242, val loss 1.6931\nstep 33600: train loss 1.3314, val loss 1.6835\nstep 33700: train loss 1.3401, val loss 1.6757\nstep 33800: train loss 1.3370, val loss 1.6800\nstep 33900: train loss 1.3295, val loss 1.6915\nstep 34000: train loss 1.3469, val loss 1.6868\nstep 34100: train loss 1.3261, val loss 1.6976\nstep 34200: train loss 1.3245, val loss 1.6902\nstep 34300: train loss 1.3362, val loss 1.6844\nstep 34400: train loss 1.3393, val loss 1.6820\nstep 34500: train loss 1.3422, val loss 1.6948\nstep 34600: train loss 1.3357, val loss 1.6826\nstep 34700: train loss 1.3371, val loss 1.6902\nstep 34800: train loss 1.3395, val loss 1.6925\nstep 34900: train loss 1.3431, val loss 1.6924\nstep 35000: train loss 1.3353, val loss 1.6945\nstep 35100: train loss 1.3551, val loss 1.6771\nstep 35200: train loss 1.3355, val loss 1.6922\nstep 35300: train loss 1.3201, val loss 1.6934\nstep 35400: train loss 1.3376, val loss 1.6904\nstep 35500: train loss 1.3517, val loss 1.6776\nstep 35600: train loss 1.3347, val loss 1.6900\nstep 35700: train loss 1.3449, val loss 1.6921\nstep 35800: train loss 1.3367, val loss 1.6786\nstep 35900: train loss 1.3254, val loss 1.6781\nstep 36000: train loss 1.3311, val loss 1.6913\nstep 36100: train loss 1.3261, val loss 1.6885\nstep 36200: train loss 1.3147, val loss 1.6840\nstep 36300: train loss 1.3282, val loss 1.6805\nstep 36400: train loss 1.3347, val loss 1.6930\nstep 36500: train loss 1.3244, val loss 1.6940\nstep 36600: train loss 1.3309, val loss 1.6808\nstep 36700: train loss 1.3365, val loss 1.6919\nstep 36800: train loss 1.3379, val loss 1.6757\nstep 36900: train loss 1.3247, val loss 1.6960\nstep 37000: train loss 1.3371, val loss 1.6778\nstep 37100: train loss 1.3134, val loss 1.6815\nstep 37200: train loss 1.3337, val loss 1.6869\nstep 37300: train loss 1.3299, val loss 1.6830\nstep 37400: train loss 1.3524, val loss 1.6952\nstep 37500: train loss 1.3048, val loss 1.6956\nstep 37600: train loss 1.3394, val loss 1.6822\nstep 37700: train loss 1.3312, val loss 1.6835\nstep 37800: train loss 1.3389, val loss 1.6812\nstep 37900: train loss 1.3296, val loss 1.6720\nstep 38000: train loss 1.3431, val loss 1.6774\nstep 38100: train loss 1.3361, val loss 1.6996\nstep 38200: train loss 1.3130, val loss 1.6753\nstep 38300: train loss 1.3265, val loss 1.6863\nstep 38400: train loss 1.3279, val loss 1.6862\nstep 38500: train loss 1.3424, val loss 1.6742\nstep 38600: train loss 1.3381, val loss 1.6862\nstep 38700: train loss 1.3307, val loss 1.6803\nstep 38800: train loss 1.3378, val loss 1.6722\nstep 38900: train loss 1.3368, val loss 1.6839\nstep 39000: train loss 1.3406, val loss 1.6739\nstep 39100: train loss 1.3256, val loss 1.6966\nstep 39200: train loss 1.3454, val loss 1.6732\nstep 39300: train loss 1.3000, val loss 1.6885\nstep 39400: train loss 1.3217, val loss 1.6833\nstep 39500: train loss 1.3143, val loss 1.6865\nstep 39600: train loss 1.3243, val loss 1.6735\nstep 39700: train loss 1.3403, val loss 1.6829\nstep 39800: train loss 1.3373, val loss 1.6802\nstep 39900: train loss 1.3256, val loss 1.6708\nstep 39999: train loss 1.3362, val loss 1.6761\n","output_type":"stream"}]},{"cell_type":"code","source":"# # generate from the model\n# # Define input string\n# input_string = \"hello\"\n# # Encode input string into character indices\n# input_indices = encode(input_string)\n# # Convert list of indices into a tensor with shape (1, len(input_indices))\n# context = torch.tensor([input_indices], dtype=torch.long, device=device)\ncontext = torch.zeros((1, 1), dtype=torch.long, device=device)\n# Generate text using the modified context\nprint(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n","metadata":{"execution":{"iopub.status.busy":"2023-05-01T10:01:32.874041Z","iopub.execute_input":"2023-05-01T10:01:32.874873Z","iopub.status.idle":"2023-05-01T10:01:36.123679Z","shell.execute_reply.started":"2023-05-01T10:01:32.874833Z","shell.execute_reply":"2023-05-01T10:01:36.122489Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"\nThe worksTand. In militant alone humine rubbe as for outebook before an\neyes windom the choice of this         \n          \n           IN SHIGHTHENGAl - NE RUN'T                   T'CHALLA\n               He Ad)\n              I have a HOLMIN all of this stycops her gio enough. 1120-hen Tribe cuning as ifferior\npeople to me walloured for\nbe behavioro Gun hardly forces, and you both for me,\n          outside okban monked power dlays on Carris of the chay are find.\n    \"IRE ! HIDEN      3 CONTINUOUS)\n","output_type":"stream"}]},{"cell_type":"code","source":"# import urllib.request\n\n# # Download the tiny shakespeare dataset\n# url = \"https://raw.githubusercontent.com/AMANDHAKAR191/dlProject/main/dataset_coversation.txt\"\n# filename = \"dataset_coversation.txt\"\n\n# urllib.request.urlretrieve(url, filename)\n\n# # Read the dataset to inspect it\n# with open(filename, 'r', encoding='utf-8') as f:\n#     text = f.read()\n\n# # Print the length of the dataset in characters and the first 1000 characters\n# print(\"Length of dataset in characters: \", len(text))\n# print(text[:1000])\n","metadata":{"execution":{"iopub.status.busy":"2023-05-01T09:37:17.198326Z","iopub.execute_input":"2023-05-01T09:37:17.199483Z","iopub.status.idle":"2023-05-01T09:37:17.204492Z","shell.execute_reply.started":"2023-05-01T09:37:17.199439Z","shell.execute_reply":"2023-05-01T09:37:17.203096Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}