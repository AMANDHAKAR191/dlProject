{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-08T13:46:29.900492Z","iopub.execute_input":"2023-11-08T13:46:29.900942Z","iopub.status.idle":"2023-11-08T13:46:29.923776Z","shell.execute_reply.started":"2023-11-08T13:46:29.900911Z","shell.execute_reply":"2023-11-08T13:46:29.922866Z"},"trusted":true},"execution_count":70,"outputs":[{"name":"stdout","text":"/kaggle/input/face-data/static/faces/aman_123/aman_33.jpg\n/kaggle/input/face-data/static/faces/aman_123/aman_5.jpg\n/kaggle/input/face-data/static/faces/aman_123/aman_44.jpg\n/kaggle/input/face-data/static/faces/aman_123/aman_41.jpg\n/kaggle/input/face-data/static/faces/aman_123/aman_1.jpg\n/kaggle/input/face-data/static/faces/aman_123/aman_29.jpg\n/kaggle/input/face-data/static/faces/aman_123/aman_25.jpg\n/kaggle/input/face-data/static/faces/aman_123/aman_12.jpg\n/kaggle/input/face-data/static/faces/aman_123/aman_32.jpg\n/kaggle/input/face-data/static/faces/aman_123/aman_49.jpg\n/kaggle/input/face-data/static/faces/aman_123/aman_6.jpg\n/kaggle/input/face-data/static/faces/aman_123/aman_0.jpg\n/kaggle/input/face-data/static/faces/aman_123/aman_31.jpg\n/kaggle/input/face-data/static/faces/aman_123/aman_28.jpg\n/kaggle/input/face-data/static/faces/aman_123/aman_45.jpg\n/kaggle/input/face-data/static/faces/aman_123/aman_34.jpg\n/kaggle/input/face-data/static/faces/aman_123/aman_48.jpg\n/kaggle/input/face-data/static/faces/aman_123/aman_46.jpg\n/kaggle/input/face-data/static/faces/aman_123/aman_38.jpg\n/kaggle/input/face-data/static/faces/aman_123/aman_15.jpg\n/kaggle/input/face-data/static/faces/aman_123/aman_14.jpg\n/kaggle/input/face-data/static/faces/aman_123/aman_13.jpg\n/kaggle/input/face-data/static/faces/aman_123/aman_24.jpg\n/kaggle/input/face-data/static/faces/aman_123/aman_27.jpg\n/kaggle/input/face-data/static/faces/aman_123/aman_7.jpg\n/kaggle/input/face-data/static/faces/aman_123/aman_17.jpg\n/kaggle/input/face-data/static/faces/aman_123/aman_39.jpg\n/kaggle/input/face-data/static/faces/aman_123/aman_19.jpg\n/kaggle/input/face-data/static/faces/aman_123/aman_18.jpg\n/kaggle/input/face-data/static/faces/aman_123/aman_4.jpg\n/kaggle/input/face-data/static/faces/aman_123/aman_22.jpg\n/kaggle/input/face-data/static/faces/aman_123/aman_8.jpg\n/kaggle/input/face-data/static/faces/aman_123/aman_2.jpg\n/kaggle/input/face-data/static/faces/aman_123/aman_23.jpg\n/kaggle/input/face-data/static/faces/aman_123/aman_43.jpg\n/kaggle/input/face-data/static/faces/aman_123/aman_16.jpg\n/kaggle/input/face-data/static/faces/aman_123/aman_20.jpg\n/kaggle/input/face-data/static/faces/aman_123/aman_35.jpg\n/kaggle/input/face-data/static/faces/aman_123/aman_11.jpg\n/kaggle/input/face-data/static/faces/aman_123/aman_10.jpg\n/kaggle/input/face-data/static/faces/aman_123/aman_3.jpg\n/kaggle/input/face-data/static/faces/aman_123/aman_42.jpg\n/kaggle/input/face-data/static/faces/aman_123/aman_26.jpg\n/kaggle/input/face-data/static/faces/aman_123/aman_30.jpg\n/kaggle/input/face-data/static/faces/aman_123/aman_37.jpg\n/kaggle/input/face-data/static/faces/aman_123/aman_21.jpg\n/kaggle/input/face-data/static/faces/aman_123/aman_36.jpg\n/kaggle/input/face-data/static/faces/aman_123/aman_9.jpg\n/kaggle/input/face-data/static/faces/aman_123/aman_47.jpg\n/kaggle/input/face-data/static/faces/aman_123/aman_40.jpg\n/kaggle/input/face-data/static/faces/sai_123/sai_46.jpg\n/kaggle/input/face-data/static/faces/sai_123/sai_9.jpg\n/kaggle/input/face-data/static/faces/sai_123/sai_43.jpg\n/kaggle/input/face-data/static/faces/sai_123/sai_27.jpg\n/kaggle/input/face-data/static/faces/sai_123/sai_20.jpg\n/kaggle/input/face-data/static/faces/sai_123/sai_32.jpg\n/kaggle/input/face-data/static/faces/sai_123/sai_47.jpg\n/kaggle/input/face-data/static/faces/sai_123/sai_10.jpg\n/kaggle/input/face-data/static/faces/sai_123/sai_42.jpg\n/kaggle/input/face-data/static/faces/sai_123/sai_23.jpg\n/kaggle/input/face-data/static/faces/sai_123/sai_34.jpg\n/kaggle/input/face-data/static/faces/sai_123/sai_4.jpg\n/kaggle/input/face-data/static/faces/sai_123/sai_17.jpg\n/kaggle/input/face-data/static/faces/sai_123/sai_14.jpg\n/kaggle/input/face-data/static/faces/sai_123/sai_35.jpg\n/kaggle/input/face-data/static/faces/sai_123/sai_8.jpg\n/kaggle/input/face-data/static/faces/sai_123/sai_28.jpg\n/kaggle/input/face-data/static/faces/sai_123/sai_15.jpg\n/kaggle/input/face-data/static/faces/sai_123/sai_40.jpg\n/kaggle/input/face-data/static/faces/sai_123/sai_48.jpg\n/kaggle/input/face-data/static/faces/sai_123/sai_16.jpg\n/kaggle/input/face-data/static/faces/sai_123/sai_19.jpg\n/kaggle/input/face-data/static/faces/sai_123/sai_33.jpg\n/kaggle/input/face-data/static/faces/sai_123/sai_12.jpg\n/kaggle/input/face-data/static/faces/sai_123/sai_25.jpg\n/kaggle/input/face-data/static/faces/sai_123/sai_38.jpg\n/kaggle/input/face-data/static/faces/sai_123/sai_36.jpg\n/kaggle/input/face-data/static/faces/sai_123/sai_13.jpg\n/kaggle/input/face-data/static/faces/sai_123/sai_21.jpg\n/kaggle/input/face-data/static/faces/sai_123/sai_30.jpg\n/kaggle/input/face-data/static/faces/sai_123/sai_31.jpg\n/kaggle/input/face-data/static/faces/sai_123/sai_1.jpg\n/kaggle/input/face-data/static/faces/sai_123/sai_2.jpg\n/kaggle/input/face-data/static/faces/sai_123/sai_24.jpg\n/kaggle/input/face-data/static/faces/sai_123/sai_26.jpg\n/kaggle/input/face-data/static/faces/sai_123/sai_39.jpg\n/kaggle/input/face-data/static/faces/sai_123/sai_29.jpg\n/kaggle/input/face-data/static/faces/sai_123/sai_6.jpg\n/kaggle/input/face-data/static/faces/sai_123/sai_3.jpg\n/kaggle/input/face-data/static/faces/sai_123/sai_11.jpg\n/kaggle/input/face-data/static/faces/sai_123/sai_0.jpg\n/kaggle/input/face-data/static/faces/sai_123/sai_22.jpg\n/kaggle/input/face-data/static/faces/sai_123/sai_37.jpg\n/kaggle/input/face-data/static/faces/sai_123/sai_45.jpg\n/kaggle/input/face-data/static/faces/sai_123/sai_49.jpg\n/kaggle/input/face-data/static/faces/sai_123/sai_44.jpg\n/kaggle/input/face-data/static/faces/sai_123/sai_41.jpg\n/kaggle/input/face-data/static/faces/sai_123/sai_7.jpg\n/kaggle/input/face-data/static/faces/sai_123/sai_18.jpg\n/kaggle/input/face-data/static/faces/sai_123/sai_5.jpg\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Activation, Dense, Flatten, BatchNormalization, Conv2D, MaxPool2D, GlobalAveragePooling2D\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.metrics import categorical_crossentropy\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator","metadata":{"execution":{"iopub.status.busy":"2023-11-08T13:46:31.415830Z","iopub.execute_input":"2023-11-08T13:46:31.417037Z","iopub.status.idle":"2023-11-08T13:46:31.423455Z","shell.execute_reply.started":"2023-11-08T13:46:31.416994Z","shell.execute_reply":"2023-11-08T13:46:31.422216Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"# model = Sequential([\n#     Dense(units=16, input_shape=(1,), activation='relu'),\n#     Dense(units=32, activation='relu'),\n#     Dense(units=2, activation='softmax')\n# ])","metadata":{"execution":{"iopub.status.busy":"2023-11-08T13:46:36.228156Z","iopub.execute_input":"2023-11-08T13:46:36.228695Z","iopub.status.idle":"2023-11-08T13:46:36.236123Z","shell.execute_reply.started":"2023-11-08T13:46:36.228661Z","shell.execute_reply":"2023-11-08T13:46:36.233573Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"code","source":"# model.summary()","metadata":{"execution":{"iopub.status.busy":"2023-11-08T13:46:41.757692Z","iopub.execute_input":"2023-11-08T13:46:41.758057Z","iopub.status.idle":"2023-11-08T13:46:41.764114Z","shell.execute_reply.started":"2023-11-08T13:46:41.758029Z","shell.execute_reply":"2023-11-08T13:46:41.763030Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"code","source":"# model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics='accuracy')","metadata":{"execution":{"iopub.status.busy":"2023-11-08T13:46:46.467893Z","iopub.execute_input":"2023-11-08T13:46:46.468243Z","iopub.status.idle":"2023-11-08T13:46:46.473184Z","shell.execute_reply.started":"2023-11-08T13:46:46.468216Z","shell.execute_reply":"2023-11-08T13:46:46.471693Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"code","source":"# import os\nfrom PIL import Image\n# import numpy as np\n\n# Define the directory path\ndirectory = \"/kaggle/input/face-data/static/faces/aman_123/\"\n\n# Initialize an empty list to store the images\nimages_aman = []\n\n# List all files in the directory\nfile_list = os.listdir(directory)\n\n# Iterate through the files and load them as images\nfor filename in file_list:\n    # Check if the file is an image (you can further filter by file extension)\n    if filename.endswith(\".jpg\"):\n        # Construct the full file path\n        file_path = os.path.join(directory, filename)\n        \n        # Open and preprocess the image\n        img = Image.open(file_path)\n        img = img.resize((224, 224))  # Resize the image to your desired dimensions\n        img = np.array(img)  # Convert to a NumPy array\n        images_aman.append(img)\n\n# Convert the list of images into a NumPy array\nimages_array_aman = np.array(images_aman)\n\n# Add a batch dimension to the images\nimages_array_aman = np.expand_dims(images_array_aman, axis=0)\n\n# Check the shape of the array (should be num_images x height x width x channels)\nprint(\"Shape of the images array:\", images_array_aman.shape)\nimages_array_aman[0][0].shape","metadata":{"execution":{"iopub.status.busy":"2023-11-08T13:47:02.168024Z","iopub.execute_input":"2023-11-08T13:47:02.168338Z","iopub.status.idle":"2023-11-08T13:47:02.498378Z","shell.execute_reply.started":"2023-11-08T13:47:02.168317Z","shell.execute_reply":"2023-11-08T13:47:02.496190Z"},"trusted":true},"execution_count":76,"outputs":[{"name":"stdout","text":"Shape of the images array: (1, 50, 224, 224, 3)\n","output_type":"stream"},{"execution_count":76,"output_type":"execute_result","data":{"text/plain":"(224, 224, 3)"},"metadata":{}}]},{"cell_type":"code","source":"# import os\n# from PIL import Image\n# import numpy as np\n\n# Define the directory path\ndirectory = \"/kaggle/input/face-data/static/faces/sai_123/\"\n\n# Initialize an empty list to store the images\nimages_sai = []\n\n# List all files in the directory\nfile_list = os.listdir(directory)\n\n# Iterate through the files and load them as images\nfor filename in file_list:\n    # Check if the file is an image (you can further filter by file extension)\n    if filename.endswith(\".jpg\"):\n        # Construct the full file path\n        file_path = os.path.join(directory, filename)\n        \n        # Open and preprocess the image\n        img = Image.open(file_path)\n        img = img.resize((224, 224))  # Resize the image to your desired dimensions\n        img = np.array(img)  # Convert to a NumPy array\n        images_sai.append(img)\n\n# Convert the list of images into a NumPy array\nimages_array_sai = np.array(images_sai)\n\n# Add a batch dimension to the images\nimages_array_sai = np.expand_dims(images_array_sai, axis=0)\n\n# Check the shape of the array (should be num_images x height x width x channels)\nprint(\"Shape of the images array:\", images_array_sai.shape)\nimages_array_sai[0][0].shape","metadata":{"execution":{"iopub.status.busy":"2023-11-08T13:47:08.795678Z","iopub.execute_input":"2023-11-08T13:47:08.796185Z","iopub.status.idle":"2023-11-08T13:47:09.141296Z","shell.execute_reply.started":"2023-11-08T13:47:08.796145Z","shell.execute_reply":"2023-11-08T13:47:09.140302Z"},"trusted":true},"execution_count":77,"outputs":[{"name":"stdout","text":"Shape of the images array: (1, 50, 224, 224, 3)\n","output_type":"stream"},{"execution_count":77,"output_type":"execute_result","data":{"text/plain":"(224, 224, 3)"},"metadata":{}}]},{"cell_type":"code","source":"mobile = tf.keras.applications.mobilenet.MobileNet(weights=\"imagenet\",include_top=False)\n# mobile.summary()","metadata":{"execution":{"iopub.status.busy":"2023-11-08T13:47:16.808676Z","iopub.execute_input":"2023-11-08T13:47:16.809129Z","iopub.status.idle":"2023-11-08T13:47:17.497804Z","shell.execute_reply.started":"2023-11-08T13:47:16.809093Z","shell.execute_reply":"2023-11-08T13:47:17.496685Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"code","source":"x = GlobalAveragePooling2D()(mobile.output)\nlinear_vector = Dense(10, activation='linear')(x)\nmodel = Model(inputs=mobile.input, outputs=linear_vector)\n\nfeature_ex = mobile.layers[-5].output\nmodel = Model(inputs=mobile.input, outputs = feature_ex)\n\nfor layer in model.layers[:-23]:\n    layer.trainable = False","metadata":{"execution":{"iopub.status.busy":"2023-11-08T13:47:19.851452Z","iopub.execute_input":"2023-11-08T13:47:19.851870Z","iopub.status.idle":"2023-11-08T13:47:19.892952Z","shell.execute_reply.started":"2023-11-08T13:47:19.851842Z","shell.execute_reply":"2023-11-08T13:47:19.890756Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"code","source":"linear_vectors1 = model.predict(images_array_aman[0])\nlinear_vectors3 = model.predict(images_array_aman[0])\nlinear_vectors2 = model.predict(images_array_sai[0])\n# linear_vectors.get_weights()\n# print(linear_vectors)","metadata":{"execution":{"iopub.status.busy":"2023-11-08T13:56:49.922338Z","iopub.execute_input":"2023-11-08T13:56:49.922739Z","iopub.status.idle":"2023-11-08T13:56:53.347460Z","shell.execute_reply.started":"2023-11-08T13:56:49.922713Z","shell.execute_reply":"2023-11-08T13:56:53.346705Z"},"trusted":true},"execution_count":89,"outputs":[{"name":"stdout","text":"2/2 [==============================] - 1s 233ms/step\n2/2 [==============================] - 1s 238ms/step\n2/2 [==============================] - 1s 283ms/step\n","output_type":"stream"}]},{"cell_type":"code","source":"# import numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Assuming linear_vectors1 and linear_vectors2 are two sets of linear vectors\n# Each set has shape (batch_size, vector_length)\n\n# Flatten the arrays to remove the batch dimension\nflat_vectors1 = linear_vectors1.reshape(linear_vectors1.shape[0], -1)\nflat_vectors2 = linear_vectors2.reshape(linear_vectors2.shape[0], -1)\n\n# Normalize the vectors\nnorm_vectors1 = flat_vectors1 / np.linalg.norm(flat_vectors1, axis=1)[:, np.newaxis]\nnorm_vectors2 = flat_vectors2 / np.linalg.norm(flat_vectors2, axis=1)[:, np.newaxis]\n\n# Compute cosine similarity between the two sets of vectors\nsimilarity = cosine_similarity(norm_vectors1, norm_vectors2)\n\n# Define a threshold to determine if the vectors are matching\nthreshold = 0.99  # Adjust the threshold as needed\n\n# Check if any pair of vectors has a similarity greater than the threshold\nmatch = (similarity >= threshold).any()\n\nif match:\n    print(\"The feature vectors are from similar images.\")\nelse:\n    print(\"The feature vectors are not from similar images.\")\n","metadata":{"execution":{"iopub.status.busy":"2023-11-08T13:58:00.599353Z","iopub.execute_input":"2023-11-08T13:58:00.599699Z","iopub.status.idle":"2023-11-08T13:58:00.629519Z","shell.execute_reply.started":"2023-11-08T13:58:00.599676Z","shell.execute_reply":"2023-11-08T13:58:00.628519Z"},"trusted":true},"execution_count":93,"outputs":[{"name":"stdout","text":"The feature vectors are not from similar images.\n","output_type":"stream"}]},{"cell_type":"code","source":"model.fit(x=train_data, epochs=10, verbose=0)","metadata":{"execution":{"iopub.status.busy":"2023-11-08T12:36:23.629736Z","iopub.execute_input":"2023-11-08T12:36:23.630107Z","iopub.status.idle":"2023-11-08T12:36:24.342395Z","shell.execute_reply.started":"2023-11-08T12:36:23.630078Z","shell.execute_reply":"2023-11-08T12:36:24.341010Z"},"trusted":true},"execution_count":24,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n","File \u001b[0;32m/tmp/__autograph_generated_fileaiuaixmn.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n","\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1052, in train_step\n        self._validate_target_and_loss(y, loss)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1016, in _validate_target_and_loss\n        raise ValueError(\n\n    ValueError: No loss found. You may have forgotten to provide a `loss` argument in the `compile()` method.\n"],"ename":"ValueError","evalue":"in user code:\n\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1052, in train_step\n        self._validate_target_and_loss(y, loss)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1016, in _validate_target_and_loss\n        raise ValueError(\n\n    ValueError: No loss found. You may have forgotten to provide a `loss` argument in the `compile()` method.\n","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}